{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "from gensim.models import doc2vec\n",
    "import os.path as osp\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch import nn,optim\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from embeddings_reproduction import embedding_tools\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "file_dir='data/'\n",
    "task='C17ORF85_Baltz2012'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_to_kmers(seq, k=3, overlap=False, **kwargs):\n",
    "    N = len(seq)\n",
    "    if overlap:\n",
    "        return [[seq[i:i+k] for i in range(N - k + 1)]]\n",
    "    else:\n",
    "        return [[seq[i:i+k] for i in range(j, N - k + 1, k)]\n",
    "                for j in range(k)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(doc2vec_file,seq, k=5, overlap=False, norm=True, steps=5):\n",
    "    \"\"\" Infer embeddings in one pass using a gensim doc2vec model.\n",
    "\n",
    "    Parameters:\n",
    "        doc2vec_file (str): file pointing to saved doc2vec model\n",
    "        seqs (iterable): sequences to infer\n",
    "        k (int) default 3\n",
    "        overlap (Boolean) default False\n",
    "        norm (Boolean) default True\n",
    "        steps (int): number of steps during inference. Default 5.\n",
    "\n",
    "    Returns:\n",
    "        numpy ndarray where each row is the embedding for one sequence.\n",
    "    \"\"\"\n",
    "    \n",
    "    as_kmer = []+seq_to_kmers(seq, k=k, overlap=overlap)\n",
    "    as_kmer=as_kmer[0]\n",
    "    model = doc2vec.Doc2Vec.load(doc2vec_file)\n",
    "    #print(as_kmer[0])\n",
    "    vector_ret=model.infer_vector(as_kmer, steps=steps)\n",
    "    return vector_ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bases = ['A', 'C', 'G', 'U']\n",
    "base_dict = {'A': 0, 'C': 1, 'G': 2, 'U': 3}\n",
    "bases_len = len(bases)\n",
    "num_feature = 271 #total number of features\n",
    "def convert_to_index(str,word_len):\n",
    "    output_index = 0\n",
    "    for i in range(word_len):\n",
    "        output_index = output_index * bases_len + base_dict[str[i]]\n",
    "    return output_index\n",
    "\n",
    "def extract_features(line):\n",
    "    line2=line\n",
    "    for i in 'agctu\\n':\n",
    "        line2 = line2.replace(i, '')\n",
    "    line = line.upper().rstrip()\n",
    "    line = line.replace('T', 'U')\n",
    "    line2 = line2.replace('T','U')\n",
    "   #line = line.replace('N','')\n",
    "   #line2 = line2.replace('N','')\n",
    "    final_output=[]#get_embeddings('outputs/docvec_models/2_virus_5_6.pkl',line).tolist()\n",
    "    final_output.extend(get_embeddings('outputs/docvec_models/0_virus_5_6.pkl',line2).tolist())\n",
    "    for word_len in [1,2,3]:\n",
    "        output_count_list = [0 for i in range(bases_len ** word_len)]\n",
    "        for i in range(len(line) - word_len + 1):\n",
    "            output_count_list[convert_to_index(line[i: i + word_len],word_len)] += 1\n",
    "        output_count_list2 = [0 for i in range(bases_len ** word_len)]\n",
    "        for i in range(len(line2)-word_len+1):\n",
    "            output_count_list2[convert_to_index(line2[i:i+word_len],word_len)] +=1\n",
    "        final_output.extend(output_count_list)\n",
    "        final_output.extend(output_count_list2)\n",
    "    for word_len in [4,5,6]:\n",
    "        output_count_list = [0 for i in range(bases_len ** 2)]\n",
    "        for i in range(len(line) - word_len):\n",
    "            output_count_list[convert_to_index(line[i]+line[i + word_len],2)] += 1\n",
    "        output_count_list2 = [0 for i in range(bases_len ** 2)]\n",
    "        for i in range(len(line2)-word_len):\n",
    "              output_count_list2[convert_to_index(line2[i]+line2[i+word_len],2)] +=1\n",
    "        final_output.extend(output_count_list)\n",
    "        final_output.extend(output_count_list2)\n",
    "    final_output.append(len(line2))\n",
    "    final_output.append(math.log(len(line2)))\n",
    "    final_output.append(int(len(line2) % 3 == 0))\n",
    "    stop_codons = ['UAG', 'UAA', 'UGA']\n",
    "    stop_codon_features = [0,0,0,0]\n",
    "    for stop_codon_num in range(len(stop_codons)):\n",
    "        tmp_arr = [m.start() for m in re.finditer(stop_codons[stop_codon_num], line2)]\n",
    "        tmp_arr_div3 = [i for i in tmp_arr if i % 3 == 0]\n",
    "        stop_codon_features[stop_codon_num]=int(len(tmp_arr_div3) > 0)\n",
    "        stop_codon_features[3]|=stop_codon_features[stop_codon_num]\n",
    "    final_output.extend(stop_codon_features)\n",
    "    return final_output\n",
    "\n",
    "\n",
    "def load_dataset(task,is_load):\n",
    "    x_train=[]\n",
    "    x_valid=[]\n",
    "    x_test=[]\n",
    "    y_train=[]\n",
    "    y_valid=[]\n",
    "    y_test=[]\n",
    "    N=0\n",
    "    filename=file_dir+task+'.train.positives.fa'\n",
    "    for line in open(filename, \"r\"):\n",
    "        if line[0] == '>':\n",
    "            continue\n",
    "        else:\n",
    "            if ('n' in line or 'N' in line):\n",
    "                continue\n",
    "            else:\n",
    "                N+=1\n",
    "    select_list = list(range(N))\n",
    "    valid_set=random.sample(select_list,(int)(N/10))\n",
    "    filename=file_dir+task+'.train.positives.fa'\n",
    "    num=0\n",
    "    for line in open(filename, \"r\"):\n",
    "        if line[0] == '>':\n",
    "            continue\n",
    "        else:\n",
    "            if ('n' in line or 'N' in line):\n",
    "                continue\n",
    "            else:\n",
    "                num+=1\n",
    "                if(num in valid_set):\n",
    "                    x_valid.append(extract_features(line.strip('\\n').strip('\\r')))\n",
    "                    y_valid.append(1.0)\n",
    "                else:\n",
    "                    x_train.append(extract_features(line.strip('\\n').strip('\\r')))\n",
    "                    y_train.append(1.0)\n",
    "                    \n",
    "                \n",
    "    filename=file_dir+task+'.train.negatives.fa'\n",
    "    num=0\n",
    "    for line in open(filename, \"r\"):\n",
    "        if line[0] == '>':\n",
    "            continue\n",
    "        else:\n",
    "            if ('n' in line or 'N' in line):\n",
    "                continue\n",
    "            else:\n",
    "                num+=1\n",
    "                if(num in valid_set):\n",
    "                    x_valid.append(extract_features(line.strip('\\n').strip('\\r')))\n",
    "                    y_valid.append(0)\n",
    "                else:\n",
    "                    x_train.append(extract_features(line.strip('\\n').strip('\\r')))\n",
    "                    y_train.append(0)\n",
    "                    \n",
    "    filename=file_dir+task+'.ls.positives.fa'\n",
    "    for line in open(filename, \"r\"):\n",
    "        if line[0] == '>':\n",
    "            continue\n",
    "        else:\n",
    "            if ('n' in line or 'N' in line):\n",
    "                continue\n",
    "            else:\n",
    "                x_test.append(extract_features(line.strip('\\n').strip('\\r')))\n",
    "                y_test.append(1.0)\n",
    "    filename=file_dir+task+'.ls.negatives.fa'\n",
    "    for line in open(filename, \"r\"):\n",
    "        if line[0] == '>':\n",
    "            continue\n",
    "        else:\n",
    "            if ('n' in line or 'N' in line):\n",
    "                continue\n",
    "            else:\n",
    "                x_test.append(extract_features(line.strip('\\n').strip('\\r')))\n",
    "                y_test.append(0)\n",
    "    return np.array(x_train),np.array(y_train),np.array(x_valid),np.array(y_valid),np.array(x_test),np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train,x_valid,y_valid,x_test,y_test=load_dataset(task,is_load=False)\n",
    "\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "\n",
    "print(x_valid.shape)\n",
    "print(y_valid.shape)\n",
    "\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "x_train=torch.from_numpy(x_train).float()\n",
    "y_train=torch.from_numpy(y_train).float()\n",
    "\n",
    "x_valid=torch.from_numpy(x_valid).float()\n",
    "y_valid=torch.from_numpy(y_valid).float()\n",
    "\n",
    "x_test=torch.from_numpy(x_test).float()\n",
    "y_test=torch.from_numpy(y_test).float()\n",
    "\n",
    "y_train = y_train.unsqueeze(1)\n",
    "y_test = y_test.unsqueeze(1)\n",
    "y_valid = y_valid.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=1e-4\n",
    "feature_num=335\n",
    "class nnNet(nn.Module):\n",
    "    def __init__(self, in_dim, n_hidden_1, out_dim):\n",
    "        super(nnNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(nn.Linear(in_dim, n_hidden_1))\n",
    "        self.layer2=nn.Sequential(nn.Linear(n_hidden_1, n_hidden_1))\n",
    "        self.layer3 = nn.Sequential(nn.Linear(n_hidden_1, out_dim))\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer3(x)\n",
    "        return torch.sigmoid(x)\n",
    "model=nnNet(feature_num,64,1)\n",
    "criterion =nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate,weight_decay=10 ** (-5.0))\n",
    "\n",
    "num_epoch=5000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best=0\n",
    "wh=0\n",
    "for epoch in range(num_epoch):\n",
    "    out=model(x_train)\n",
    "    loss=criterion(out,y_train)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print_loss=loss.data.item()\n",
    "    train_auc=roc_auc_score(y_train,out.detach().numpy())\n",
    "    out_valid=model(x_valid)\n",
    "    valid_auc=roc_auc_score(y_valid,out_valid.detach().numpy())\n",
    "    \n",
    "    if(valid_auc>best):\n",
    "        best=valid_auc\n",
    "        wh=epoch\n",
    "        torch.save({'model_state_dict': model.state_dict()},task+'local_best_model.pth')\n",
    "    if(epoch%10==0):\n",
    "        print(print_loss,train_auc,valid_auc)\n",
    "model.load_state_dict(torch.load(task+'local_best_model.pth')['model_state_dict'])\n",
    "out_test=model(x_test)\n",
    "test_auc=roc_auc_score(y_test,out_test.detach().numpy())\n",
    "print(best,test_auc)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
