{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "from gensim.models import doc2vec\n",
    "import os.path as osp\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch import nn,optim\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from embeddings_reproduction import embedding_tools\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "file_dir='data/'\n",
    "task='C17ORF85_Baltz2012'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_to_kmers(seq, k=3, overlap=False, **kwargs):\n",
    "    N = len(seq)\n",
    "    if overlap:\n",
    "        return [[seq[i:i+k] for i in range(N - k + 1)]]\n",
    "    else:\n",
    "        return [[seq[i:i+k] for i in range(j, N - k + 1, k)]\n",
    "                for j in range(k)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(doc2vec_file,seq, k=5, overlap=False, norm=True, steps=5):\n",
    "    \"\"\" Infer embeddings in one pass using a gensim doc2vec model.\n",
    "\n",
    "    Parameters:\n",
    "        doc2vec_file (str): file pointing to saved doc2vec model\n",
    "        seqs (iterable): sequences to infer\n",
    "        k (int) default 3\n",
    "        overlap (Boolean) default False\n",
    "        norm (Boolean) default True\n",
    "        steps (int): number of steps during inference. Default 5.\n",
    "\n",
    "    Returns:\n",
    "        numpy ndarray where each row is the embedding for one sequence.\n",
    "    \"\"\"\n",
    "    \n",
    "    as_kmer = []+seq_to_kmers(seq, k=k, overlap=overlap)\n",
    "    as_kmer=as_kmer[0]\n",
    "    model = doc2vec.Doc2Vec.load(doc2vec_file)\n",
    "    #print(as_kmer[0])\n",
    "    vector_ret=model.infer_vector(as_kmer, steps=steps)\n",
    "    return vector_ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bases = ['A', 'C', 'G', 'U']\n",
    "base_dict = {'A': 0, 'C': 1, 'G': 2, 'U': 3}\n",
    "bases_len = len(bases)\n",
    "num_feature = 271 #total number of features\n",
    "def convert_to_index(str,word_len):\n",
    "    output_index = 0\n",
    "    for i in range(word_len):\n",
    "        output_index = output_index * bases_len + base_dict[str[i]]\n",
    "    return output_index\n",
    "\n",
    "def extract_features(line):\n",
    "    line2=line\n",
    "    for i in 'agctu\\n':\n",
    "        line2 = line2.replace(i, '')\n",
    "    line = line.upper().rstrip()\n",
    "    line = line.replace('T', 'U')\n",
    "    line2 = line2.replace('T','U')\n",
    "   #line = line.replace('N','')\n",
    "   #line2 = line2.replace('N','')\n",
    "    final_output=get_embeddings('outputs/docvec_models/2_virus_5_6.pkl',line).tolist()\n",
    "   # final_output.extend(get_embeddings('outputs/docvec_models/0_virus_5_6.pkl',line2).tolist())\n",
    "    for word_len in [1,2,3]:\n",
    "        output_count_list = [0 for i in range(bases_len ** word_len)]\n",
    "        for i in range(len(line) - word_len + 1):\n",
    "            output_count_list[convert_to_index(line[i: i + word_len],word_len)] += 1\n",
    "        output_count_list2 = [0 for i in range(bases_len ** word_len)]\n",
    "        for i in range(len(line2)-word_len+1):\n",
    "            output_count_list2[convert_to_index(line2[i:i+word_len],word_len)] +=1\n",
    "        final_output.extend(output_count_list)\n",
    "        final_output.extend(output_count_list2)\n",
    "    for word_len in [4,5,6]:\n",
    "        output_count_list = [0 for i in range(bases_len ** 2)]\n",
    "        for i in range(len(line) - word_len):\n",
    "            output_count_list[convert_to_index(line[i]+line[i + word_len],2)] += 1\n",
    "        output_count_list2 = [0 for i in range(bases_len ** 2)]\n",
    "        for i in range(len(line2)-word_len):\n",
    "              output_count_list2[convert_to_index(line2[i]+line2[i+word_len],2)] +=1\n",
    "        final_output.extend(output_count_list)\n",
    "        final_output.extend(output_count_list2)\n",
    "    final_output.append(len(line2))\n",
    "    final_output.append(math.log(len(line2)))\n",
    "    final_output.append(int(len(line2) % 3 == 0))\n",
    "    stop_codons = ['UAG', 'UAA', 'UGA']\n",
    "    stop_codon_features = [0,0,0,0]\n",
    "    for stop_codon_num in range(len(stop_codons)):\n",
    "        tmp_arr = [m.start() for m in re.finditer(stop_codons[stop_codon_num], line2)]\n",
    "        tmp_arr_div3 = [i for i in tmp_arr if i % 3 == 0]\n",
    "        stop_codon_features[stop_codon_num]=int(len(tmp_arr_div3) > 0)\n",
    "        stop_codon_features[3]|=stop_codon_features[stop_codon_num]\n",
    "    final_output.extend(stop_codon_features)\n",
    "    return final_output\n",
    "\n",
    "\n",
    "def load_dataset(task,is_load):\n",
    "    x_train=[]\n",
    "    x_valid=[]\n",
    "    x_test=[]\n",
    "    y_train=[]\n",
    "    y_valid=[]\n",
    "    y_test=[]\n",
    "    N=0\n",
    "    filename=file_dir+task+'.train.positives.fa'\n",
    "    for line in open(filename, \"r\"):\n",
    "        if line[0] == '>':\n",
    "            continue\n",
    "        else:\n",
    "            if ('n' in line or 'N' in line):\n",
    "                continue\n",
    "            else:\n",
    "                N+=1\n",
    "    select_list = list(range(N))\n",
    "    valid_set=random.sample(select_list,(int)(N/10))\n",
    "    filename=file_dir+task+'.train.positives.fa'\n",
    "    num=0\n",
    "    for line in open(filename, \"r\"):\n",
    "        if line[0] == '>':\n",
    "            continue\n",
    "        else:\n",
    "            if ('n' in line or 'N' in line):\n",
    "                continue\n",
    "            else:\n",
    "                num+=1\n",
    "                if(num in valid_set):\n",
    "                    x_valid.append(extract_features(line.strip('\\n').strip('\\r')))\n",
    "                    y_valid.append(1.0)\n",
    "                else:\n",
    "                    x_train.append(extract_features(line.strip('\\n').strip('\\r')))\n",
    "                    y_train.append(1.0)\n",
    "                    \n",
    "                \n",
    "    filename=file_dir+task+'.train.negatives.fa'\n",
    "    num=0\n",
    "    for line in open(filename, \"r\"):\n",
    "        if line[0] == '>':\n",
    "            continue\n",
    "        else:\n",
    "            if ('n' in line or 'N' in line):\n",
    "                continue\n",
    "            else:\n",
    "                num+=1\n",
    "                if(num in valid_set):\n",
    "                    x_valid.append(extract_features(line.strip('\\n').strip('\\r')))\n",
    "                    y_valid.append(0)\n",
    "                else:\n",
    "                    x_train.append(extract_features(line.strip('\\n').strip('\\r')))\n",
    "                    y_train.append(0)\n",
    "                    \n",
    "    filename=file_dir+task+'.ls.positives.fa'\n",
    "    for line in open(filename, \"r\"):\n",
    "        if line[0] == '>':\n",
    "            continue\n",
    "        else:\n",
    "            if ('n' in line or 'N' in line):\n",
    "                continue\n",
    "            else:\n",
    "                x_test.append(extract_features(line.strip('\\n').strip('\\r')))\n",
    "                y_test.append(1.0)\n",
    "    filename=file_dir+task+'.ls.negatives.fa'\n",
    "    for line in open(filename, \"r\"):\n",
    "        if line[0] == '>':\n",
    "            continue\n",
    "        else:\n",
    "            if ('n' in line or 'N' in line):\n",
    "                continue\n",
    "            else:\n",
    "                x_test.append(extract_features(line.strip('\\n').strip('\\r')))\n",
    "                y_test.append(0)\n",
    "    return np.array(x_train),np.array(y_train),np.array(x_valid),np.array(y_valid),np.array(x_test),np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1fbae4998664>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_load\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-b0470315ac08>\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(task, is_load)\u001b[0m\n\u001b[1;32m     86\u001b[0m                     \u001b[0my_valid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m                     \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-b0470315ac08>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(line)\u001b[0m\n\u001b[1;32m     19\u001b[0m    \u001b[0;31m#line = line.replace('N','')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m    \u001b[0;31m#line2 = line2.replace('N','')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mfinal_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'outputs/docvec_models/2_virus_5_6.pkl'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m    \u001b[0;31m# final_output.extend(get_embeddings('outputs/docvec_models/0_virus_5_6.pkl',line2).tolist())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword_len\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-7c0c6ebf1b29>\u001b[0m in \u001b[0;36mget_embeddings\u001b[0;34m(doc2vec_file, seq, k, overlap, norm, steps)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mas_kmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mseq_to_kmers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverlap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverlap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mas_kmer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_kmer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDoc2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc2vec_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;31m#print(as_kmer[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mvector_ret\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mas_kmer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spyder/lib/python3.6/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \"\"\"\n\u001b[1;32m   1112\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1113\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDoc2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1114\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model saved using code from earlier Gensim Version. Re-loading old model in a compatible way.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spyder/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m         \"\"\"\n\u001b[0;32m-> 1244\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseWordEmbeddingsModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ns_exponent'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mns_exponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spyder/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname_or_handle, **kwargs)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \"\"\"\n\u001b[0;32m--> 603\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseAny2VecModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spyder/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSaveLoad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapt_by_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spyder/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m   1382\u001b[0m         \u001b[0;31m# Because of loading from S3 load can't be used (missing readline in smart_open)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x_train,y_train,x_valid,y_valid,x_test,y_test=load_dataset(task,is_load=False)\n",
    "\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "\n",
    "print(x_valid.shape)\n",
    "print(y_valid.shape)\n",
    "\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "x_train=torch.from_numpy(x_train).float()\n",
    "y_train=torch.from_numpy(y_train).float()\n",
    "\n",
    "x_valid=torch.from_numpy(x_valid).float()\n",
    "y_valid=torch.from_numpy(y_valid).float()\n",
    "\n",
    "x_test=torch.from_numpy(x_test).float()\n",
    "y_test=torch.from_numpy(y_test).float()\n",
    "\n",
    "y_train = y_train.unsqueeze(1)\n",
    "y_test = y_test.unsqueeze(1)\n",
    "y_valid = y_valid.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=1e-4\n",
    "feature_num=335\n",
    "class nnNet(nn.Module):\n",
    "    def __init__(self, in_dim, n_hidden_1, out_dim):\n",
    "        super(nnNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(nn.Linear(in_dim, n_hidden_1))\n",
    "        self.layer2=nn.Sequential(nn.Linear(n_hidden_1, n_hidden_1))\n",
    "        self.layer3 = nn.Sequential(nn.Linear(n_hidden_1, out_dim))\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer3(x)\n",
    "        return torch.sigmoid(x)\n",
    "model=nnNet(feature_num,64,1)\n",
    "criterion =nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate,weight_decay=10 ** (-5.0))\n",
    "\n",
    "num_epoch=5000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best=0\n",
    "wh=0\n",
    "for epoch in range(num_epoch):\n",
    "    out=model(x_train)\n",
    "    loss=criterion(out,y_train)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print_loss=loss.data.item()\n",
    "    train_auc=roc_auc_score(y_train,out.detach().numpy())\n",
    "    out_valid=model(x_valid)\n",
    "    valid_auc=roc_auc_score(y_valid,out_valid.detach().numpy())\n",
    "    \n",
    "    if(valid_auc>best):\n",
    "        best=valid_auc\n",
    "        wh=epoch\n",
    "        torch.save({'model_state_dict': model.state_dict()},task+'global_best_model.pth')\n",
    "    if(epoch%10==0):\n",
    "        print(print_loss,train_auc,valid_auc)\n",
    "model.load_state_dict(torch.load(task+'global_best_model.pth')['model_state_dict'])\n",
    "out_test=model(x_test)\n",
    "test_auc=roc_auc_score(y_test,out_test.detach().numpy())\n",
    "print(best,test_auc)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
